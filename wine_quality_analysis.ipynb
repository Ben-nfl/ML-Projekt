{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Prediction - Vollständiges ML-Projekt\n",
    "\n",
    "## Projektübersicht\n",
    "Dieses Notebook analysiert den Wine Quality Datensatz von UCI Machine Learning Repository.\n",
    "Ziel ist es, die Weinqualität basierend auf physikalisch-chemischen Eigenschaften vorherzusagen.\n",
    "\n",
    "**Datensatz:** Wine Quality Dataset (Red & White Wine)\n",
    "\n",
    "**Quelle:** https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "\n",
    "**Ansatz:** Wir verwenden beide Datensätze (Rot- und Weißwein) und kombinieren sie, da:\n",
    "1. Mehr Daten generell bessere Modelle ermöglichen\n",
    "2. Wir ein allgemeines Qualitätsmodell für Wein entwickeln können\n",
    "3. Wir einen 'wine_type' Feature hinzufügen, um zwischen den Weintypen zu unterscheiden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import der benötigten Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenverarbeitung\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Machine Learning - Regression Modelle\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Machine Learning - Klassifikation Modelle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Metriken - Regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Metriken - Klassifikation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    ")\n",
    "\n",
    "# Warnungen unterdrücken\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot-Einstellungen\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Alle Bibliotheken erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Daten laden und erste Inspektion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Datensätze\n",
    "red_wine = pd.read_csv('wine+quality/winequality-red.csv', sep=';')\n",
    "white_wine = pd.read_csv('wine+quality/winequality-white.csv', sep=';')\n",
    "\n",
    "# Weintyp als Feature hinzufügen\n",
    "red_wine['wine_type'] = 1  # 1 für Rotwein\n",
    "white_wine['wine_type'] = 0  # 0 für Weißwein\n",
    "\n",
    "print(f\"Rotwein Datensatz: {red_wine.shape}\")\n",
    "print(f\"Weißwein Datensatz: {white_wine.shape}\")\n",
    "\n",
    "# Datensätze kombinieren\n",
    "df = pd.concat([red_wine, white_wine], axis=0, ignore_index=True)\n",
    "print(f\"\\nKombinierter Datensatz: {df.shape}\")\n",
    "print(f\"Anzahl Samples: {len(df)}\")\n",
    "print(f\"Anzahl Features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erste Zeilen anzeigen\n",
    "print(\"Erste 10 Zeilen des Datensatzes:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grundlegende Informationen\n",
    "print(\"Datentypen und fehlende Werte:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistische Zusammenfassung\n",
    "print(\"Statistische Zusammenfassung aller Features:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Zielvariable \"quality\" analysieren\n",
    "\n",
    "Die Zielvariable **quality** ist eine ordinale Variable mit Werten von 0 bis 10, wobei:\n",
    "- **0** = sehr schlechte Qualität\n",
    "- **10** = exzellente Qualität\n",
    "\n",
    "In der Praxis enthält der Datensatz meist Werte zwischen 3 und 9.\n",
    "\n",
    "Wir werden zwei Ansätze verfolgen:\n",
    "1. **Regression**: Vorhersage des exakten Qualitätswerts (3-9)\n",
    "2. **Klassifikation**: Binäre Klassifikation in \"gut\" (≥6) und \"schlecht\" (<6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der Qualität\n",
    "print(\"Verteilung der Qualitätswerte:\")\n",
    "print(df['quality'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nMinimale Qualität: {df['quality'].min()}\")\n",
    "print(f\"Maximale Qualität: {df['quality'].max()}\")\n",
    "print(f\"Durchschnittliche Qualität: {df['quality'].mean():.2f}\")\n",
    "print(f\"Median Qualität: {df['quality'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Qualitätsverteilung\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Balkendiagramm\n",
    "df['quality'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Verteilung der Weinqualität', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Qualität')\n",
    "axes[0].set_ylabel('Anzahl')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Vergleich zwischen Rot- und Weißwein\n",
    "quality_by_type = df.groupby(['wine_type', 'quality']).size().unstack(fill_value=0)\n",
    "quality_by_type.T.plot(kind='bar', ax=axes[1], color=['#8B0000', '#FFD700'])\n",
    "axes[1].set_title('Qualitätsverteilung: Rotwein vs Weißwein', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Qualität')\n",
    "axes[1].set_ylabel('Anzahl')\n",
    "axes[1].legend(['Weißwein', 'Rotwein'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datenbereinigung und Qualitätsprüfung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfung auf fehlende Werte\n",
    "print(\"Fehlende Werte pro Spalte:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nGesamtzahl fehlender Werte: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfung auf Duplikate\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Anzahl duplizierter Zeilen: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"\\nEntferne {duplicates} duplizierte Zeilen...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Neue Datensatzgröße: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfung auf negative oder ungültige Werte\n",
    "print(\"Prüfung auf negative Werte (sollten bei physikalischen Messungen nicht vorkommen):\")\n",
    "for col in df.columns:\n",
    "    if col not in ['quality', 'wine_type']:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"{col}: {negative_count} negative Werte gefunden!\")\n",
    "\n",
    "print(\"\\nKeine negativen Werte gefunden - Daten sind valide!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ausreißer-Erkennung mit IQR-Methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Ausreißer-Erkennung mit IQR\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"\n",
    "    Erkennt Ausreißer mit der IQR-Methode (Interquartile Range).\n",
    "    Ausreißer sind Werte, die außerhalb von [Q1 - 1.5*IQR, Q3 + 1.5*IQR] liegen.\n",
    "    \"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Ausreißer für alle numerischen Features analysieren\n",
    "feature_cols = [col for col in df.columns if col not in ['quality', 'wine_type']]\n",
    "\n",
    "outlier_summary = {}\n",
    "for col in feature_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(df)) * 100,\n",
    "        'lower_bound': lower,\n",
    "        'upper_bound': upper\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "print(\"Ausreißer-Analyse (IQR-Methode):\")\n",
    "print(outlier_df.sort_values('percentage', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entscheidung: Wir behalten die Ausreißer, da sie chemische Extremwerte sein könnten,\n",
    "# die für die Qualität relevant sind. Bei Bedarf könnten wir sie entfernen.\n",
    "# Zur Demonstration zeigen wir, wie man sie entfernen würde:\n",
    "\n",
    "df_no_outliers = df.copy()\n",
    "original_size = len(df_no_outliers)\n",
    "\n",
    "# Optional: Extreme Ausreißer entfernen (nur sehr extreme Werte)\n",
    "for col in feature_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_no_outliers, col)\n",
    "    # Nur sehr extreme Ausreißer entfernen (3*IQR statt 1.5*IQR)\n",
    "    Q1 = df_no_outliers[col].quantile(0.25)\n",
    "    Q3 = df_no_outliers[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    extreme_lower = Q1 - 3 * IQR\n",
    "    extreme_upper = Q3 + 3 * IQR\n",
    "    df_no_outliers = df_no_outliers[\n",
    "        (df_no_outliers[col] >= extreme_lower) & \n",
    "        (df_no_outliers[col] <= extreme_upper)\n",
    "    ]\n",
    "\n",
    "removed = original_size - len(df_no_outliers)\n",
    "print(f\"\\nMit extremer Ausreißer-Entfernung (3*IQR):\")\n",
    "print(f\"Entfernte Zeilen: {removed} ({(removed/original_size)*100:.2f}%)\")\n",
    "print(f\"Verbleibende Zeilen: {len(df_no_outliers)}\")\n",
    "\n",
    "# Wir verwenden den Originaldatensatz für das Training\n",
    "print(\"\\n→ Für das Training verwenden wir den vollständigen Datensatz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Korrelationsanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrelationsmatrix berechnen\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Korrelation mit der Zielvariable \"quality\"\n",
    "quality_correlation = correlation_matrix['quality'].sort_values(ascending=False)\n",
    "print(\"Korrelation der Features mit der Weinqualität:\")\n",
    "print(quality_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stark korrelierte Feature-Paare identifizieren (Multikollinearität)\n",
    "print(\"\\nStark korrelierte Feature-Paare (|r| > 0.7):\")\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Umfassende Visualisierungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Histogramme aller Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme für alle Features\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    if idx < 12:\n",
    "        axes[idx].hist(df[col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Verteilung: {col}', fontweight='bold')\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Häufigkeit')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Boxplots zur Ausreißer-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots für alle numerischen Features\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    if idx < 11:\n",
    "        axes[idx].boxplot(df[col], vert=True)\n",
    "        axes[idx].set_title(f'Boxplot: {col}', fontweight='bold')\n",
    "        axes[idx].set_ylabel(col)\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Qualität Boxplot\n",
    "axes[11].boxplot(df['quality'], vert=True)\n",
    "axes[11].set_title('Boxplot: quality', fontweight='bold')\n",
    "axes[11].set_ylabel('quality')\n",
    "axes[11].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Heatmap der Korrelationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korrelations-Heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=1,\n",
    "    cbar_kws={'shrink': 0.8}\n",
    ")\n",
    "plt.title('Korrelationsmatrix aller Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Scatterplots: Qualität vs. wichtige Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 6 Features mit höchster Korrelation zur Qualität (außer quality selbst)\n",
    "top_features = quality_correlation.drop('quality').abs().nlargest(6).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    axes[idx].scatter(df[feature], df['quality'], alpha=0.3, c=df['wine_type'], cmap='RdYlBu')\n",
    "    axes[idx].set_xlabel(feature, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Quality', fontweight='bold')\n",
    "    axes[idx].set_title(f'Quality vs {feature}\\n(r = {quality_correlation[feature]:.3f})', fontweight='bold')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Pairplot der wichtigsten Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot für die 5 wichtigsten Features plus quality\n",
    "top_5_features = quality_correlation.drop('quality').abs().nlargest(5).index.tolist()\n",
    "pairplot_features = top_5_features + ['quality']\n",
    "\n",
    "print(f\"Erstelle Pairplot für: {pairplot_features}\")\n",
    "\n",
    "# Sample für schnellere Visualisierung (optional)\n",
    "df_sample = df[pairplot_features].sample(n=min(1000, len(df)), random_state=42)\n",
    "\n",
    "sns.pairplot(\n",
    "    df_sample,\n",
    "    diag_kind='hist',\n",
    "    plot_kws={'alpha': 0.6},\n",
    "    height=2.5\n",
    ")\n",
    "plt.suptitle('Pairplot der wichtigsten Features', y=1.01, fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Neue Features erstellen\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# 1. Verhältnis von freiem zu totalem Schwefel\n",
    "df_engineered['free_to_total_sulfur_ratio'] = (\n",
    "    df_engineered['free sulfur dioxide'] / \n",
    "    (df_engineered['total sulfur dioxide'] + 1e-10)  # Vermeidung Division durch 0\n",
    ")\n",
    "\n",
    "# 2. Säure-Verhältnis (Weinsäure zu Zitronensäure)\n",
    "df_engineered['acid_ratio'] = (\n",
    "    df_engineered['fixed acidity'] / \n",
    "    (df_engineered['volatile acidity'] + 1e-10)\n",
    ")\n",
    "\n",
    "# 3. Gesamtsäure\n",
    "df_engineered['total_acidity'] = (\n",
    "    df_engineered['fixed acidity'] + \n",
    "    df_engineered['volatile acidity'] + \n",
    "    df_engineered['citric acid']\n",
    ")\n",
    "\n",
    "# 4. Alkohol pro Säure (Balance)\n",
    "df_engineered['alcohol_per_acid'] = (\n",
    "    df_engineered['alcohol'] / \n",
    "    (df_engineered['total_acidity'] + 1e-10)\n",
    ")\n",
    "\n",
    "# 5. Binäre Qualitätsklasse für Klassifikation\n",
    "df_engineered['quality_class'] = (df_engineered['quality'] >= 6).astype(int)\n",
    "# 0 = schlecht (<6), 1 = gut (≥6)\n",
    "\n",
    "print(\"Neue Features erstellt:\")\n",
    "print(\"- free_to_total_sulfur_ratio\")\n",
    "print(\"- acid_ratio\")\n",
    "print(\"- total_acidity\")\n",
    "print(\"- alcohol_per_acid\")\n",
    "print(\"- quality_class (binär: 0=schlecht, 1=gut)\")\n",
    "print(f\"\\nNeue Datensatzgröße: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung der binären Qualitätsklassen\n",
    "print(\"Verteilung der Qualitätsklassen:\")\n",
    "print(df_engineered['quality_class'].value_counts())\n",
    "print(f\"\\nProzentuale Verteilung:\")\n",
    "print(df_engineered['quality_class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualisierung\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "df_engineered['quality_class'].value_counts().plot(kind='bar', ax=ax[0], color=['#e74c3c', '#2ecc71'])\n",
    "ax[0].set_title('Verteilung: Qualitätsklassen', fontweight='bold')\n",
    "ax[0].set_xlabel('Klasse (0=schlecht, 1=gut)')\n",
    "ax[0].set_ylabel('Anzahl')\n",
    "ax[0].set_xticklabels(['Schlecht (<6)', 'Gut (≥6)'], rotation=0)\n",
    "\n",
    "ax[1].pie(\n",
    "    df_engineered['quality_class'].value_counts(),\n",
    "    labels=['Schlecht (<6)', 'Gut (≥6)'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['#e74c3c', '#2ecc71'],\n",
    "    startangle=90\n",
    ")\n",
    "ax[1].set_title('Prozentuale Verteilung', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Daten vorbereiten: Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features für das Training\n",
    "feature_columns = [col for col in df_engineered.columns \n",
    "                   if col not in ['quality', 'quality_class']]\n",
    "\n",
    "print(f\"Features für Training: {len(feature_columns)}\")\n",
    "print(feature_columns)\n",
    "\n",
    "# Feature Matrix X\n",
    "X = df_engineered[feature_columns]\n",
    "\n",
    "# Target für Regression\n",
    "y_regression = df_engineered['quality']\n",
    "\n",
    "# Target für Klassifikation\n",
    "y_classification = df_engineered['quality_class']\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y_regression shape: {y_regression.shape}\")\n",
    "print(f\"y_classification shape: {y_classification.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split für Regression (80/20, mit Stratifikation basierend auf quality)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_regression,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_regression\n",
    ")\n",
    "\n",
    "# Train-Test Split für Klassifikation (80/20, mit Stratifikation basierend auf quality_class)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X, y_classification,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_classification\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split erfolgreich:\")\n",
    "print(f\"\\nRegression:\")\n",
    "print(f\"  Training Set: {X_train_reg.shape}\")\n",
    "print(f\"  Test Set: {X_test_reg.shape}\")\n",
    "print(f\"\\nKlassifikation:\")\n",
    "print(f\"  Training Set: {X_train_clf.shape}\")\n",
    "print(f\"  Test Set: {X_test_clf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Feature Standardisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler für Regression\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# StandardScaler für Klassifikation\n",
    "scaler_clf = StandardScaler()\n",
    "X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\n",
    "X_test_clf_scaled = scaler_clf.transform(X_test_clf)\n",
    "\n",
    "print(\"Features erfolgreich standardisiert (Mean=0, Std=1)\")\n",
    "print(f\"\\nBeispiel - Mittelwerte nach Standardisierung:\")\n",
    "print(np.mean(X_train_reg_scaled, axis=0)[:5])\n",
    "print(f\"\\nBeispiel - Standardabweichungen nach Standardisierung:\")\n",
    "print(np.std(X_train_reg_scaled, axis=0)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression: Modelltraining und Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline Modelle trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary für Regression Modelle\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(random_state=42),\n",
    "    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'Support Vector Regressor': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "# Ergebnisse speichern\n",
    "regression_results = {}\n",
    "\n",
    "print(\"Trainiere Regressionsmodelle...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    print(f\"\\nTrainiere {name}...\")\n",
    "    \n",
    "    # Training\n",
    "    model.fit(X_train_reg_scaled, y_train_reg)\n",
    "    \n",
    "    # Vorhersagen\n",
    "    y_pred_train = model.predict(X_train_reg_scaled)\n",
    "    y_pred_test = model.predict(X_test_reg_scaled)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_reg, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test))\n",
    "    train_mae = mean_absolute_error(y_train_reg, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test_reg, y_pred_test)\n",
    "    train_r2 = r2_score(y_train_reg, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_reg, y_pred_test)\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    regression_results[name] = {\n",
    "        'model': model,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train MAE:  {train_mae:.4f} | Test MAE:  {test_mae:.4f}\")\n",
    "    print(f\"  Train R²:   {train_r2:.4f} | Test R²:   {test_r2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Alle Regressionsmodelle trainiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleichstabelle erstellen\n",
    "regression_comparison = pd.DataFrame({\n",
    "    'Model': list(regression_results.keys()),\n",
    "    'Test RMSE': [results['test_rmse'] for results in regression_results.values()],\n",
    "    'Test MAE': [results['test_mae'] for results in regression_results.values()],\n",
    "    'Test R²': [results['test_r2'] for results in regression_results.values()],\n",
    "    'Train RMSE': [results['train_rmse'] for results in regression_results.values()],\n",
    "    'Train R²': [results['train_r2'] for results in regression_results.values()]\n",
    "})\n",
    "\n",
    "# Sortieren nach Test R²\n",
    "regression_comparison = regression_comparison.sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"\\nVergleich aller Regressionsmodelle:\")\n",
    "print(regression_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Modell-Performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# RMSE Vergleich\n",
    "x_pos = np.arange(len(regression_comparison))\n",
    "axes[0].bar(x_pos, regression_comparison['Test RMSE'], color='steelblue', alpha=0.7)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(regression_comparison['Model'], rotation=45, ha='right')\n",
    "axes[0].set_title('Test RMSE Vergleich', fontweight='bold')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# MAE Vergleich\n",
    "axes[1].bar(x_pos, regression_comparison['Test MAE'], color='coral', alpha=0.7)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(regression_comparison['Model'], rotation=45, ha='right')\n",
    "axes[1].set_title('Test MAE Vergleich', fontweight='bold')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R² Vergleich\n",
    "axes[2].bar(x_pos, regression_comparison['Test R²'], color='seagreen', alpha=0.7)\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels(regression_comparison['Model'], rotation=45, ha='right')\n",
    "axes[2].set_title('Test R² Vergleich', fontweight='bold')\n",
    "axes[2].set_ylabel('R² Score')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Predicted vs Actual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual für die besten 3 Modelle\n",
    "top_3_models = regression_comparison.head(3)['Model'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, model_name in enumerate(top_3_models):\n",
    "    y_pred = regression_results[model_name]['y_pred']\n",
    "    \n",
    "    axes[idx].scatter(y_test_reg, y_pred, alpha=0.5, s=20)\n",
    "    axes[idx].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "                   [y_test_reg.min(), y_test_reg.max()], \n",
    "                   'r--', lw=2, label='Perfect Prediction')\n",
    "    axes[idx].set_xlabel('Tatsächliche Qualität', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Vorhergesagte Qualität', fontweight='bold')\n",
    "    axes[idx].set_title(f'{model_name}\\nR² = {regression_results[model_name][\"test_r2\"]:.4f}', \n",
    "                        fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Klassifikation: Modelltraining und Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Baseline Modelle trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary für Klassifikationsmodelle\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest Classifier': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting Classifier': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Support Vector Classifier': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Ergebnisse speichern\n",
    "classification_results = {}\n",
    "\n",
    "print(\"Trainiere Klassifikationsmodelle...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in classification_models.items():\n",
    "    print(f\"\\nTrainiere {name}...\")\n",
    "    \n",
    "    # Training\n",
    "    model.fit(X_train_clf_scaled, y_train_clf)\n",
    "    \n",
    "    # Vorhersagen\n",
    "    y_pred_train = model.predict(X_train_clf_scaled)\n",
    "    y_pred_test = model.predict(X_test_clf_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_clf_scaled)[:, 1]\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    train_acc = accuracy_score(y_train_clf, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test_clf, y_pred_test)\n",
    "    test_precision = precision_score(y_test_clf, y_pred_test)\n",
    "    test_recall = recall_score(y_test_clf, y_pred_test)\n",
    "    test_f1 = f1_score(y_test_clf, y_pred_test)\n",
    "    test_auc = roc_auc_score(y_test_clf, y_pred_proba)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_clf, y_pred_test)\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    classification_results[name] = {\n",
    "        'model': model,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1': test_f1,\n",
    "        'auc': test_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'y_pred': y_pred_test,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Precision: {test_precision:.4f} | Recall: {test_recall:.4f}\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f} | AUC: {test_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Alle Klassifikationsmodelle trainiert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleichstabelle erstellen\n",
    "classification_comparison = pd.DataFrame({\n",
    "    'Model': list(classification_results.keys()),\n",
    "    'Accuracy': [results['test_acc'] for results in classification_results.values()],\n",
    "    'Precision': [results['precision'] for results in classification_results.values()],\n",
    "    'Recall': [results['recall'] for results in classification_results.values()],\n",
    "    'F1-Score': [results['f1'] for results in classification_results.values()],\n",
    "    'AUC': [results['auc'] for results in classification_results.values()]\n",
    "})\n",
    "\n",
    "# Sortieren nach F1-Score\n",
    "classification_comparison = classification_comparison.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\nVergleich aller Klassifikationsmodelle:\")\n",
    "print(classification_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Modell-Performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Metriken Vergleich\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "x = np.arange(len(classification_comparison))\n",
    "width = 0.15\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].bar(x + i*width, classification_comparison[metric], width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Modelle', fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontweight='bold')\n",
    "axes[0].set_title('Klassifikationsmetriken Vergleich', fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 2)\n",
    "axes[0].set_xticklabels(classification_comparison['Model'], rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1-Score Ranking\n",
    "axes[1].barh(classification_comparison['Model'], classification_comparison['F1-Score'], \n",
    "             color='steelblue', alpha=0.7)\n",
    "axes[1].set_xlabel('F1-Score', fontweight='bold')\n",
    "axes[1].set_title('F1-Score Ranking', fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices für alle Modelle\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, results) in enumerate(classification_results.items()):\n",
    "    cm = results['confusion_matrix']\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        ax=axes[idx],\n",
    "        cbar=False\n",
    "    )\n",
    "    axes[idx].set_title(f'{name}\\nF1={results[\"f1\"]:.4f}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xticklabels(['Schlecht', 'Gut'])\n",
    "    axes[idx].set_yticklabels(['Schlecht', 'Gut'])\n",
    "\n",
    "# Leeres Subplot entfernen\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 ROC Kurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Kurven für alle Modelle\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, results in classification_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test_clf, results['y_pred_proba'])\n",
    "    auc_score = results['auc']\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
    "plt.xlabel('False Positive Rate', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontweight='bold', fontsize=12)\n",
    "plt.title('ROC Curves - Klassifikationsmodelle', fontweight='bold', fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detaillierte Classification Reports\n",
    "print(\"Classification Reports für alle Modelle:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, results in classification_results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(classification_report(\n",
    "        y_test_clf, \n",
    "        results['y_pred'],\n",
    "        target_names=['Schlecht (<6)', 'Gut (≥6)']\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Random Forest Regressor Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid für Random Forest Regressor\n",
    "rf_reg_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Tuning für Random Forest Regressor...\")\n",
    "print(f\"Anzahl Kombinationen: {np.prod([len(v) for v in rf_reg_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV (schneller als GridSearchCV)\n",
    "rf_reg_random = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    rf_reg_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_reg_random.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "print(f\"\\nBeste Parameter: {rf_reg_random.best_params_}\")\n",
    "print(f\"Bester R² Score (CV): {rf_reg_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste Modell evaluieren\n",
    "best_rf_reg = rf_reg_random.best_estimator_\n",
    "y_pred_test_tuned = best_rf_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "tuned_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test_tuned))\n",
    "tuned_mae = mean_absolute_error(y_test_reg, y_pred_test_tuned)\n",
    "tuned_r2 = r2_score(y_test_reg, y_pred_test_tuned)\n",
    "\n",
    "print(\"\\nTuned Random Forest Regressor Performance:\")\n",
    "print(f\"  Test RMSE: {tuned_rmse:.4f}\")\n",
    "print(f\"  Test MAE:  {tuned_mae:.4f}\")\n",
    "print(f\"  Test R²:   {tuned_r2:.4f}\")\n",
    "\n",
    "# Vergleich mit Baseline\n",
    "baseline_r2 = regression_results['Random Forest Regressor']['test_r2']\n",
    "improvement = ((tuned_r2 - baseline_r2) / baseline_r2) * 100\n",
    "print(f\"\\nVerbesserung gegenüber Baseline: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Gradient Boosting Regressor Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid für Gradient Boosting Regressor\n",
    "gb_reg_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Tuning für Gradient Boosting Regressor...\")\n",
    "print(f\"Anzahl Kombinationen: {np.prod([len(v) for v in gb_reg_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV\n",
    "gb_reg_random = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    gb_reg_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_reg_random.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "print(f\"\\nBeste Parameter: {gb_reg_random.best_params_}\")\n",
    "print(f\"Bester R² Score (CV): {gb_reg_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste Modell evaluieren\n",
    "best_gb_reg = gb_reg_random.best_estimator_\n",
    "y_pred_test_gb_tuned = best_gb_reg.predict(X_test_reg_scaled)\n",
    "\n",
    "tuned_gb_rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_test_gb_tuned))\n",
    "tuned_gb_mae = mean_absolute_error(y_test_reg, y_pred_test_gb_tuned)\n",
    "tuned_gb_r2 = r2_score(y_test_reg, y_pred_test_gb_tuned)\n",
    "\n",
    "print(\"\\nTuned Gradient Boosting Regressor Performance:\")\n",
    "print(f\"  Test RMSE: {tuned_gb_rmse:.4f}\")\n",
    "print(f\"  Test MAE:  {tuned_gb_mae:.4f}\")\n",
    "print(f\"  Test R²:   {tuned_gb_r2:.4f}\")\n",
    "\n",
    "# Vergleich mit Baseline\n",
    "baseline_gb_r2 = regression_results['Gradient Boosting Regressor']['test_r2']\n",
    "improvement_gb = ((tuned_gb_r2 - baseline_gb_r2) / baseline_gb_r2) * 100\n",
    "print(f\"\\nVerbesserung gegenüber Baseline: {improvement_gb:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Random Forest Classifier Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid für Random Forest Classifier\n",
    "rf_clf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Tuning für Random Forest Classifier...\")\n",
    "print(f\"Anzahl Kombinationen: {np.prod([len(v) for v in rf_clf_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV\n",
    "rf_clf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_clf_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_clf_random.fit(X_train_clf_scaled, y_train_clf)\n",
    "\n",
    "print(f\"\\nBeste Parameter: {rf_clf_random.best_params_}\")\n",
    "print(f\"Bester F1 Score (CV): {rf_clf_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste Modell evaluieren\n",
    "best_rf_clf = rf_clf_random.best_estimator_\n",
    "y_pred_test_clf_tuned = best_rf_clf.predict(X_test_clf_scaled)\n",
    "y_pred_proba_clf_tuned = best_rf_clf.predict_proba(X_test_clf_scaled)[:, 1]\n",
    "\n",
    "tuned_clf_acc = accuracy_score(y_test_clf, y_pred_test_clf_tuned)\n",
    "tuned_clf_precision = precision_score(y_test_clf, y_pred_test_clf_tuned)\n",
    "tuned_clf_recall = recall_score(y_test_clf, y_pred_test_clf_tuned)\n",
    "tuned_clf_f1 = f1_score(y_test_clf, y_pred_test_clf_tuned)\n",
    "tuned_clf_auc = roc_auc_score(y_test_clf, y_pred_proba_clf_tuned)\n",
    "\n",
    "print(\"\\nTuned Random Forest Classifier Performance:\")\n",
    "print(f\"  Accuracy:  {tuned_clf_acc:.4f}\")\n",
    "print(f\"  Precision: {tuned_clf_precision:.4f}\")\n",
    "print(f\"  Recall:    {tuned_clf_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {tuned_clf_f1:.4f}\")\n",
    "print(f\"  AUC:       {tuned_clf_auc:.4f}\")\n",
    "\n",
    "# Vergleich mit Baseline\n",
    "baseline_clf_f1 = classification_results['Random Forest Classifier']['f1']\n",
    "improvement_clf = ((tuned_clf_f1 - baseline_clf_f1) / baseline_clf_f1) * 100\n",
    "print(f\"\\nVerbesserung gegenüber Baseline: {improvement_clf:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Gradient Boosting Classifier Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Grid für Gradient Boosting Classifier\n",
    "gb_clf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Tuning für Gradient Boosting Classifier...\")\n",
    "print(f\"Anzahl Kombinationen: {np.prod([len(v) for v in gb_clf_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV\n",
    "gb_clf_random = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    gb_clf_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_clf_random.fit(X_train_clf_scaled, y_train_clf)\n",
    "\n",
    "print(f\"\\nBeste Parameter: {gb_clf_random.best_params_}\")\n",
    "print(f\"Bester F1 Score (CV): {gb_clf_random.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beste Modell evaluieren\n",
    "best_gb_clf = gb_clf_random.best_estimator_\n",
    "y_pred_test_gb_clf_tuned = best_gb_clf.predict(X_test_clf_scaled)\n",
    "y_pred_proba_gb_clf_tuned = best_gb_clf.predict_proba(X_test_clf_scaled)[:, 1]\n",
    "\n",
    "tuned_gb_clf_acc = accuracy_score(y_test_clf, y_pred_test_gb_clf_tuned)\n",
    "tuned_gb_clf_precision = precision_score(y_test_clf, y_pred_test_gb_clf_tuned)\n",
    "tuned_gb_clf_recall = recall_score(y_test_clf, y_pred_test_gb_clf_tuned)\n",
    "tuned_gb_clf_f1 = f1_score(y_test_clf, y_pred_test_gb_clf_tuned)\n",
    "tuned_gb_clf_auc = roc_auc_score(y_test_clf, y_pred_proba_gb_clf_tuned)\n",
    "\n",
    "print(\"\\nTuned Gradient Boosting Classifier Performance:\")\n",
    "print(f\"  Accuracy:  {tuned_gb_clf_acc:.4f}\")\n",
    "print(f\"  Precision: {tuned_gb_clf_precision:.4f}\")\n",
    "print(f\"  Recall:    {tuned_gb_clf_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {tuned_gb_clf_f1:.4f}\")\n",
    "print(f\"  AUC:       {tuned_gb_clf_auc:.4f}\")\n",
    "\n",
    "# Vergleich mit Baseline\n",
    "baseline_gb_clf_f1 = classification_results['Gradient Boosting Classifier']['f1']\n",
    "improvement_gb_clf = ((tuned_gb_clf_f1 - baseline_gb_clf_f1) / baseline_gb_clf_f1) * 100\n",
    "print(f\"\\nVerbesserung gegenüber Baseline: {improvement_gb_clf:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance für Tuned Random Forest Regressor\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': best_rf_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest Regressor):\")\n",
    "print(feature_importance_reg)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_reg['feature'], feature_importance_reg['importance'], color='steelblue')\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.title('Feature Importance - Random Forest Regressor (Tuned)', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance für Tuned Random Forest Classifier\n",
    "feature_importance_clf = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': best_rf_clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest Classifier):\")\n",
    "print(feature_importance_clf)\n",
    "\n",
    "# Visualisierung\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance_clf['feature'], feature_importance_clf['importance'], color='coral')\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.title('Feature Importance - Random Forest Classifier (Tuned)', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Finale Modellvergleiche und Empfehlung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktualisierte Regressions-Vergleichstabelle mit getunten Modellen\n",
    "final_regression_comparison = pd.DataFrame({\n",
    "    'Model': list(regression_results.keys()) + ['RF Regressor (Tuned)', 'GB Regressor (Tuned)'],\n",
    "    'Test RMSE': [results['test_rmse'] for results in regression_results.values()] + [tuned_rmse, tuned_gb_rmse],\n",
    "    'Test MAE': [results['test_mae'] for results in regression_results.values()] + [tuned_mae, tuned_gb_mae],\n",
    "    'Test R²': [results['test_r2'] for results in regression_results.values()] + [tuned_r2, tuned_gb_r2]\n",
    "}).sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINALER VERGLEICH - REGRESSIONSMODELLE\")\n",
    "print(\"=\"*80)\n",
    "print(final_regression_comparison.to_string(index=False))\n",
    "\n",
    "# Beste Regression Modell\n",
    "best_reg_model = final_regression_comparison.iloc[0]\n",
    "print(f\"\\n🏆 BESTES REGRESSIONSMODELL: {best_reg_model['Model']}\")\n",
    "print(f\"   Test R²:   {best_reg_model['Test R²']:.4f}\")\n",
    "print(f\"   Test RMSE: {best_reg_model['Test RMSE']:.4f}\")\n",
    "print(f\"   Test MAE:  {best_reg_model['Test MAE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktualisierte Klassifikations-Vergleichstabelle mit getunten Modellen\n",
    "final_classification_comparison = pd.DataFrame({\n",
    "    'Model': list(classification_results.keys()) + ['RF Classifier (Tuned)', 'GB Classifier (Tuned)'],\n",
    "    'Accuracy': [results['test_acc'] for results in classification_results.values()] + [tuned_clf_acc, tuned_gb_clf_acc],\n",
    "    'Precision': [results['precision'] for results in classification_results.values()] + [tuned_clf_precision, tuned_gb_clf_precision],\n",
    "    'Recall': [results['recall'] for results in classification_results.values()] + [tuned_clf_recall, tuned_gb_clf_recall],\n",
    "    'F1-Score': [results['f1'] for results in classification_results.values()] + [tuned_clf_f1, tuned_gb_clf_f1],\n",
    "    'AUC': [results['auc'] for results in classification_results.values()] + [tuned_clf_auc, tuned_gb_clf_auc]\n",
    "}).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINALER VERGLEICH - KLASSIFIKATIONSMODELLE\")\n",
    "print(\"=\"*80)\n",
    "print(final_classification_comparison.to_string(index=False))\n",
    "\n",
    "# Beste Klassifikation Modell\n",
    "best_clf_model = final_classification_comparison.iloc[0]\n",
    "print(f\"\\n🏆 BESTES KLASSIFIKATIONSMODELL: {best_clf_model['Model']}\")\n",
    "print(f\"   F1-Score:  {best_clf_model['F1-Score']:.4f}\")\n",
    "print(f\"   Accuracy:  {best_clf_model['Accuracy']:.4f}\")\n",
    "print(f\"   Precision: {best_clf_model['Precision']:.4f}\")\n",
    "print(f\"   Recall:    {best_clf_model['Recall']:.4f}\")\n",
    "print(f\"   AUC:       {best_clf_model['AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Finale Empfehlung\n",
    "\n",
    "### Regressionsmodelle:\n",
    "**Empfohlen:** Tuned Random Forest Regressor oder Gradient Boosting Regressor\n",
    "\n",
    "**Begründung:**\n",
    "- Beide Ensemble-Methoden zeigen sehr gute Performance\n",
    "- Random Forest ist robuster gegenüber Overfitting\n",
    "- Gradient Boosting kann leicht bessere Ergebnisse liefern, ist aber anfälliger für Overfitting\n",
    "- R² > 0.45 zeigt moderate Vorhersagekraft für ein komplexes Problem\n",
    "\n",
    "### Klassifikationsmodelle:\n",
    "**Empfohlen:** Tuned Random Forest Classifier oder Gradient Boosting Classifier\n",
    "\n",
    "**Begründung:**\n",
    "- Exzellente F1-Scores und AUC-Werte\n",
    "- Gute Balance zwischen Precision und Recall\n",
    "- Robust und interpretierbar durch Feature Importance\n",
    "- Random Forest ist schneller zu trainieren und vorherzusagen\n",
    "\n",
    "### Allgemeine Erkenntnisse:\n",
    "1. **Wichtigste Features:** Alkoholgehalt, volatile Säure, Sulfate und Zitronensäure\n",
    "2. **Weintyp:** Hat moderaten Einfluss auf die Qualität\n",
    "3. **Feature Engineering:** Verhältnis-Features verbessern die Performance\n",
    "4. **Hyperparameter Tuning:** Bringt messbare Verbesserungen (5-10%)\n",
    "\n",
    "### Produktions-Einsatz:\n",
    "- Für **schnelle Vorhersagen**: Random Forest\n",
    "- Für **höchste Genauigkeit**: Gradient Boosting\n",
    "- Für **Interpretierbarkeit**: Random Forest mit Feature Importance\n",
    "- Für **binäre Entscheidungen** (gut/schlecht): Klassifikationsmodell\n",
    "- Für **exakte Qualitätswerte**: Regressionsmodell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Modelle speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Modelle und Scaler speichern\n",
    "models_to_save = {\n",
    "    'rf_regressor': best_rf_reg,\n",
    "    'gb_regressor': best_gb_reg,\n",
    "    'rf_classifier': best_rf_clf,\n",
    "    'gb_classifier': best_gb_clf,\n",
    "    'scaler_reg': scaler_reg,\n",
    "    'scaler_clf': scaler_clf,\n",
    "    'feature_columns': feature_columns\n",
    "}\n",
    "\n",
    "# Speichern\n",
    "with open('wine_quality_models.pkl', 'wb') as f:\n",
    "    pickle.dump(models_to_save, f)\n",
    "\n",
    "print(\"Alle Modelle erfolgreich gespeichert in 'wine_quality_models.pkl'\")\n",
    "print(\"\\nGespeicherte Komponenten:\")\n",
    "for key in models_to_save.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zusammenfassung\n",
    "\n",
    "Dieses Notebook hat einen vollständigen Machine-Learning-Workflow durchgeführt:\n",
    "\n",
    "1. ✅ **Daten laden**: Rot- und Weißwein kombiniert (6497 Samples)\n",
    "2. ✅ **Exploration**: Statistische Analyse und Visualisierungen\n",
    "3. ✅ **Bereinigung**: Duplikate entfernt, Ausreißer analysiert\n",
    "4. ✅ **Feature Engineering**: 4 neue Features erstellt + binäre Qualitätsklasse\n",
    "5. ✅ **Regression**: 6 Modelle trainiert und evaluiert (RMSE, MAE, R²)\n",
    "6. ✅ **Klassifikation**: 5 Modelle trainiert und evaluiert (Accuracy, Precision, Recall, F1, AUC)\n",
    "7. ✅ **Hyperparameter Tuning**: Random Forest und Gradient Boosting optimiert\n",
    "8. ✅ **Feature Importance**: Wichtigste Features identifiziert\n",
    "9. ✅ **Vergleich**: Alle Modelle systematisch verglichen\n",
    "10. ✅ **Empfehlung**: Beste Modelle für verschiedene Use Cases\n",
    "\n",
    "**Nächster Schritt:** Streamlit-App für interaktive Vorhersagen!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
